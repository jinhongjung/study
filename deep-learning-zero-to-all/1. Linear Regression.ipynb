{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Tutorial on Simple Linear Regression Using TensorFlow\n",
    "\n",
    "In this section, we look into the basic machanism of TensorFlow based on the simple example of Simple Linear Regression. In one demensional point case, the loss function of Simple LR is defined as follows:\n",
    "\n",
    "$$loss(W, b) = \\frac{1}{m}\\sum_{i=1}^{m}(H(x_i) - y_i)^{2}$$\n",
    "\n",
    "where $H(x_i) = Wx + b$ is a linear hypothesis. The following codes represents the procedure for training LR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build graph using TF operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# X and Y data\n",
    "#x_train = [1, 2, 3]\n",
    "#y_train = [1, 2, 3]\n",
    "\n",
    "# the below enables users to change data (modulize)\n",
    "X = tf.placeholder(tf.float32, shape=[None]) \n",
    "Y = tf.placeholder(tf.float32, shape=[None])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([1]), name=\"weight\") #trainable variable\n",
    "b = tf.Variable(tf.random_normal([1]), name=\"bias\")\n",
    "\n",
    "# Our hypothesis\n",
    "H = X*W + b\n",
    "\n",
    "# cost/loss function\n",
    "loss = tf.reduce_mean(tf.square(H - Y))\n",
    "\n",
    "# minimize the loss function\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# the above builds the following graph\n",
    "#                       |-- (W)\n",
    "# (train)--(loss)--(H)---\n",
    "#                       |-- (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Run/update graph and get results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 14.7652 [-0.59200692] [ 0.02296198]\n",
      "200 0.0202148 [ 0.8352651] [ 0.37448117]\n",
      "400 0.00771901 [ 0.89820373] [ 0.23140673]\n",
      "600 0.0029475 [ 0.93709612] [ 0.14299533]\n",
      "800 0.0011255 [ 0.96112919] [ 0.08836239]\n",
      "1000 0.000429774 [ 0.97598016] [ 0.05460268]\n",
      "1200 0.000164107 [ 0.98515725] [ 0.03374112]\n",
      "1400 6.26651e-05 [ 0.9908281] [ 0.02085003]\n",
      "1600 2.39283e-05 [ 0.99433231] [ 0.01288398]\n",
      "1800 9.13708e-06 [ 0.99649769] [ 0.00796155]\n",
      "2000 3.48881e-06 [ 0.99783581] [ 0.00491973]\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph in a session\n",
    "sess = tf.Session()\n",
    "\n",
    "# Initializes global variables in the graph\n",
    "sess.run(tf.global_variables_initializer()) # before using W, B, must do it\n",
    "\n",
    "# Fit the line\n",
    "for step in range(2001):\n",
    "    # trigger - train->minimize(loss)->compute(loss)->compute(H)->(W,b)\n",
    "    # sess.run(train) \n",
    "    cost_val, W_val, b_val, _ = sess.run([loss, W, b, train],\n",
    "                                     feed_dict={X:[1,2,3], Y:[1,2,3]})\n",
    "    if step % 200 == 0:\n",
    "        print(step, cost_val, W_val, b_val)\n",
    "        # print(step, sess.run(loss), sess.run(W), sess.run(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the loss function of Simple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11191f908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = [1,2,3]\n",
    "Y = [1,2,3]\n",
    "\n",
    "W = tf.placeholder(tf.float32)\n",
    "\n",
    "H = X*W # simplified\n",
    "\n",
    "cost = tf.reduce_mean(tf.square(H - Y))\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "W_val = []\n",
    "cost_val = []\n",
    "for i in range(-30, 50):\n",
    "    feed_W = i * 0.1\n",
    "    curr_cost, curr_W = sess.run([cost, W], feed_dict={W: feed_W})\n",
    "    W_val.append(curr_W)\n",
    "    cost_val.append(curr_cost)\n",
    "    \n",
    "plt.plot(W_val, cost_val)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent for Simple Linear Regression using TensorFlow\n",
    "In this section, we implement gradient descent for Simple Linear Regression based on TensorFlow. Suppose that the loss function of Simple Linear Regression without a bias term is defined as follows:\n",
    "\n",
    "$$ loss(W) = \\frac{1}{m}\\sum_{i=1}^{m}(Wx_{i} - y_{i})^{2}$$ \n",
    "\n",
    "The gradient of the loss function of Simple LR is represented as follows:\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial W} loss(W) = \\frac{2}{m}\\sum_{i=1}^{m}(Wx_{i} - y_{i})x_{i}$$\n",
    "\n",
    "Then, the gradient descent on $W$ for Simple Linear Regression is:\n",
    "\n",
    "$$ W \\leftarrow W - \\alpha \\frac{\\partial}{\\partial W} loss(W)$$\n",
    "\n",
    "where $\\alpha$ is a learning rate of the gradient descent.\n",
    "The following codes are for the gradient descent using TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0177252 [ 0.93837005]\n",
      "1 7.87787e-05 [ 0.99589133]\n",
      "2 3.50024e-07 [ 0.99972612]\n",
      "3 1.56041e-09 [ 0.9999817]\n",
      "4 7.46188e-12 [ 0.99999875]\n",
      "5 2.4869e-14 [ 0.99999994]\n",
      "6 0.0 [ 1.]\n",
      "7 0.0 [ 1.]\n",
      "8 0.0 [ 1.]\n",
      "9 0.0 [ 1.]\n",
      "10 0.0 [ 1.]\n",
      "11 0.0 [ 1.]\n",
      "12 0.0 [ 1.]\n",
      "13 0.0 [ 1.]\n",
      "14 0.0 [ 1.]\n",
      "15 0.0 [ 1.]\n",
      "16 0.0 [ 1.]\n",
      "17 0.0 [ 1.]\n",
      "18 0.0 [ 1.]\n",
      "19 0.0 [ 1.]\n",
      "20 0.0 [ 1.]\n"
     ]
    }
   ],
   "source": [
    "x_data = [1,2,3]\n",
    "y_data = [1,2,3]\n",
    "\n",
    "W = tf.Variable(tf.random_normal([1]), name='weight')\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "H = X*W # simplified\n",
    "\n",
    "cost = tf.reduce_mean(tf.square(H - Y))\n",
    "\n",
    "# define the gradient node for gradient descent\n",
    "learning_rate = 0.1\n",
    "gradient = tf.reduce_mean((W*X - Y)*X) * 2\n",
    "descent = W - learning_rate * gradient\n",
    "update = W.assign(descent)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(21):\n",
    "    sess.run(update, feed_dict={X: x_data, Y: y_data})\n",
    "    print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}), \n",
    "          sess.run(W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to modify capture the gradient of TF's optimizer \n",
    "- gvs = optimizer.compute_gradient(cost)\n",
    "- apply_gradients = optimizer.apply_gradients(gvs)\n",
    "- sess.run(apply_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial on Multi-variable Linear Regression\n",
    "In this section, we study on Multi-variable Linear Regression using TensorFlow. In this case, a data point consists of multiple number of variables. Suppose a data point (or instance) $\\mathbf{x}_{i}=[x_{i1}, \\cdots, x_{id}]$, then, a set of data points is denoted by $\\mathbf{X}=[\\mathbf{x}_{i}; \\cdots; \\mathbf{x}_{m}]$. The weight vector has the same dimension of that of $\\mathbf{x}_{i}$, i.e., $\\mathbf{w}\\in\\mathbb{R}^{d}$. Then, the hypothesis of Linear Regression is represented with bais terms as follows:\n",
    "\n",
    "$$H(\\mathbf{X}) = \\mathbf{Xw} + \\mathbf{b}$$\n",
    "\n",
    "Then, the loss function is represented as follows:\n",
    "\n",
    "$$loss(W, b) = \\frac{1}{m}||H(\\mathbf{X}) - \\mathbf{Y}||_{2}^{2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 11992.5\n",
      "500 9.0197\n",
      "1000 8.22688\n",
      "1500 7.64881\n",
      "2000 7.22387\n",
      "2500 6.90911\n",
      "3000 6.67433\n",
      "3500 6.49813\n",
      "4000 6.36513\n",
      "4500 6.26428\n",
      "Your score will be  [[ 189.16983032]]\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "import numpy as np\n",
    "\n",
    "xy = np.loadtxt('datasets/data-01-test-score.csv', delimiter=',', \n",
    "                dtype=np.float32)\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "#print(x_data.shape, x_data, len(x_data))\n",
    "#print(y_data.shape, y_data)\n",
    "\n",
    "d = x_data.shape[1]\n",
    "\n",
    "# build a graph\n",
    "X = tf.placeholder(tf.float32, shape=[None, d]) # None: flexible # of data points\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "W = tf.Variable(tf.random_normal([d, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bais')\n",
    "\n",
    "H = tf.matmul(X, W) + b\n",
    "\n",
    "loss = tf.reduce_mean(tf.square(H - Y))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# run the graph\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(5000):\n",
    "    loss_val, W_val, b_val, _ = sess.run([loss, W, b, train], \n",
    "                                         feed_dict={X: x_data, Y:y_data})\n",
    "    if i%500 == 0:\n",
    "        print(i, loss_val)\n",
    "        \n",
    "print(\"Your score will be \", sess.run(H, feed_dict={X: [[100, 70, 101]]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Next step: Queue Runner (6:50)\n",
    "# http://sdr1982.tistory.com/181\n",
    "# http://www.datascienceassn.org/content/installation-quickstart-tensorflow-anaconda-jupyter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Queue Runner \n",
    "The follow codes show how to use Queue Runner for the case when your data file is not fit into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cost:  82850.6 \n",
      "Prediction\n",
      " [[-112.65097046]\n",
      " [-153.31192017]\n",
      " [-141.67698669]\n",
      " [-156.48040771]\n",
      " [-119.51522827]\n",
      " [ -93.4522171 ]\n",
      " [-115.25369263]\n",
      " [ -82.31643677]\n",
      " [-150.03955078]\n",
      " [-145.31611633]]\n",
      "100 Cost:  98.1515 \n",
      "Prediction\n",
      " [[ 162.83824158]\n",
      " [ 178.58656311]\n",
      " [ 184.92843628]\n",
      " [ 199.39537048]\n",
      " [ 133.66014099]\n",
      " [  93.56962585]\n",
      " [ 151.17544556]\n",
      " [ 115.23575592]\n",
      " [ 160.54258728]\n",
      " [ 142.82150269]]\n",
      "200 Cost:  90.2478 \n",
      "Prediction\n",
      " [[ 162.45448303]\n",
      " [ 178.8273468 ]\n",
      " [ 184.7908783 ]\n",
      " [ 199.40455627]\n",
      " [ 133.88703918]\n",
      " [  94.06419373]\n",
      " [ 151.22171021]\n",
      " [ 115.30435944]\n",
      " [ 161.12467957]\n",
      " [ 143.77481079]]\n",
      "300 Cost:  83.0156 \n",
      "Prediction\n",
      " [[ 162.08703613]\n",
      " [ 179.05853271]\n",
      " [ 184.65951538]\n",
      " [ 199.41304016]\n",
      " [ 134.10551453]\n",
      " [  94.53766632]\n",
      " [ 151.2645874 ]\n",
      " [ 115.3673172 ]\n",
      " [ 161.68193054]\n",
      " [ 144.68608093]]\n",
      "400 Cost:  76.3974 \n",
      "Prediction\n",
      " [[ 161.73516846]\n",
      " [ 179.28048706]\n",
      " [ 184.53405762]\n",
      " [ 199.42088318]\n",
      " [ 134.31585693]\n",
      " [  94.99093628]\n",
      " [ 151.304245  ]\n",
      " [ 115.42489624]\n",
      " [ 162.2154541 ]\n",
      " [ 145.55717468]]\n",
      "500 Cost:  70.3411 \n",
      "Prediction\n",
      " [[ 161.3981781 ]\n",
      " [ 179.49359131]\n",
      " [ 184.41419983]\n",
      " [ 199.42805481]\n",
      " [ 134.5184021 ]\n",
      " [  95.42486572]\n",
      " [ 151.34083557]\n",
      " [ 115.47743988]\n",
      " [ 162.72619629]\n",
      " [ 146.38983154]]\n",
      "600 Cost:  64.7986 \n",
      "Prediction\n",
      " [[ 161.07545471]\n",
      " [ 179.69827271]\n",
      " [ 184.29974365]\n",
      " [ 199.43464661]\n",
      " [ 134.7134552 ]\n",
      " [  95.8403244 ]\n",
      " [ 151.37458801]\n",
      " [ 115.52523041]\n",
      " [ 163.21522522]\n",
      " [ 147.18579102]]\n",
      "700 Cost:  59.7261 \n",
      "Prediction\n",
      " [[ 160.76634216]\n",
      " [ 179.89479065]\n",
      " [ 184.19041443]\n",
      " [ 199.44064331]\n",
      " [ 134.90130615]\n",
      " [  96.23810577]\n",
      " [ 151.40560913]\n",
      " [ 115.56853485]\n",
      " [ 163.68344116]\n",
      " [ 147.94667053]]\n",
      "800 Cost:  55.0836 \n",
      "Prediction\n",
      " [[ 160.4703064 ]\n",
      " [ 180.08355713]\n",
      " [ 184.08599854]\n",
      " [ 199.44612122]\n",
      " [ 135.08224487]\n",
      " [  96.61897278]\n",
      " [ 151.43408203]\n",
      " [ 115.60760498]\n",
      " [ 164.13175964]\n",
      " [ 148.67401123]]\n",
      "900 Cost:  50.8343 \n",
      "Prediction\n",
      " [[ 160.18673706]\n",
      " [ 180.26486206]\n",
      " [ 183.98626709]\n",
      " [ 199.45109558]\n",
      " [ 135.25653076]\n",
      " [  96.98366547]\n",
      " [ 151.46011353]\n",
      " [ 115.64269257]\n",
      " [ 164.56103516]\n",
      " [ 149.36933899]]\n",
      "1000 Cost:  46.9449 \n",
      "Prediction\n",
      " [[ 159.91511536]\n",
      " [ 180.43899536]\n",
      " [ 183.89102173]\n",
      " [ 199.45559692]\n",
      " [ 135.42446899]\n",
      " [  97.33287048]\n",
      " [ 151.48387146]\n",
      " [ 115.67402649]\n",
      " [ 164.97212219]\n",
      " [ 150.0340271 ]]\n",
      "1100 Cost:  43.3846 \n",
      "Prediction\n",
      " [[ 159.65490723]\n",
      " [ 180.60630798]\n",
      " [ 183.80004883]\n",
      " [ 199.45962524]\n",
      " [ 135.58624268]\n",
      " [  97.66728973]\n",
      " [ 151.50546265]\n",
      " [ 115.70184326]\n",
      " [ 165.36578369]\n",
      " [ 150.66944885]]\n",
      "1200 Cost:  40.1255 \n",
      "Prediction\n",
      " [[ 159.40565491]\n",
      " [ 180.76704407]\n",
      " [ 183.71315002]\n",
      " [ 199.46325684]\n",
      " [ 135.74215698]\n",
      " [  97.98752594]\n",
      " [ 151.52505493]\n",
      " [ 115.72632599]\n",
      " [ 165.74278259]\n",
      " [ 151.27693176]]\n",
      "1300 Cost:  37.1418 \n",
      "Prediction\n",
      " [[ 159.16687012]\n",
      " [ 180.92150879]\n",
      " [ 183.63015747]\n",
      " [ 199.4664917 ]\n",
      " [ 135.89239502]\n",
      " [  98.29423523]\n",
      " [ 151.54270935]\n",
      " [ 115.74770355]\n",
      " [ 166.10385132]\n",
      " [ 151.85769653]]\n",
      "1400 Cost:  34.4104 \n",
      "Prediction\n",
      " [[ 158.93804932]\n",
      " [ 181.06988525]\n",
      " [ 183.55088806]\n",
      " [ 199.46932983]\n",
      " [ 136.03718567]\n",
      " [  98.58796692]\n",
      " [ 151.55856323]\n",
      " [ 115.76612854]\n",
      " [ 166.449646  ]\n",
      " [ 152.41287231]]\n",
      "1500 Cost:  31.9095 \n",
      "Prediction\n",
      " [[ 158.71884155]\n",
      " [ 181.2124939 ]\n",
      " [ 183.47518921]\n",
      " [ 199.47183228]\n",
      " [ 136.17675781]\n",
      " [  98.8693161 ]\n",
      " [ 151.57270813]\n",
      " [ 115.7818222 ]\n",
      " [ 166.78086853]\n",
      " [ 152.94367981]]\n",
      "1600 Cost:  29.6197 \n",
      "Prediction\n",
      " [[ 158.50880432]\n",
      " [ 181.3495636 ]\n",
      " [ 183.40289307]\n",
      " [ 199.47399902]\n",
      " [ 136.31132507]\n",
      " [  99.13877869]\n",
      " [ 151.58529663]\n",
      " [ 115.79492188]\n",
      " [ 167.09812927]\n",
      " [ 153.45114136]]\n",
      "1700 Cost:  27.5231 \n",
      "Prediction\n",
      " [[ 158.30754089]\n",
      " [ 181.48130798]\n",
      " [ 183.33384705]\n",
      " [ 199.47587585]\n",
      " [ 136.4410553 ]\n",
      " [  99.39689636]\n",
      " [ 151.59635925]\n",
      " [ 115.80561829]\n",
      " [ 167.40202332]\n",
      " [ 153.93632507]]\n",
      "1800 Cost:  25.6033 \n",
      "Prediction\n",
      " [[ 158.1146698 ]\n",
      " [ 181.60794067]\n",
      " [ 183.26789856]\n",
      " [ 199.47743225]\n",
      " [ 136.56613159]\n",
      " [  99.64415741]\n",
      " [ 151.60600281]\n",
      " [ 115.81404877]\n",
      " [ 167.69314575]\n",
      " [ 154.40016174]]\n",
      "1900 Cost:  23.8451 \n",
      "Prediction\n",
      " [[ 157.92984009]\n",
      " [ 181.72970581]\n",
      " [ 183.20492554]\n",
      " [ 199.47874451]\n",
      " [ 136.68676758]\n",
      " [  99.88102722]\n",
      " [ 151.61434937]\n",
      " [ 115.82035065]\n",
      " [ 167.97206116]\n",
      " [ 154.84368896]]\n",
      "2000 Cost:  22.2351 \n",
      "Prediction\n",
      " [[ 157.75267029]\n",
      " [ 181.84672546]\n",
      " [ 183.14476013]\n",
      " [ 199.47976685]\n",
      " [ 136.80308533]\n",
      " [ 100.1079483 ]\n",
      " [ 151.62141418]\n",
      " [ 115.82467651]\n",
      " [ 168.23924255]\n",
      " [ 155.26771545]]\n"
     ]
    }
   ],
   "source": [
    "filename_queue = tf.train.string_input_producer(['data-01-test-score.csv'], \n",
    "                                                shuffle=False, name='filename_queue')\n",
    "\n",
    "reader = tf.TextLineReader()\n",
    "key, value = reader.read(filename_queue)\n",
    "\n",
    "#Default values\n",
    "record_defaults = [[0.], [0.], [0.], [0.]]\n",
    "xy = tf.decode_csv(value, record_defaults=record_defaults)\n",
    "\n",
    "# collect batches of csv\n",
    "train_x_batch, train_y_batch = tf.train.batch([xy[0:-1], xy[-1:]], batch_size=10)\n",
    "\n",
    "# placeholder for a tensor that will be always fed\n",
    "X = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "\n",
    "H = tf.matmul(X, W) + b\n",
    "\n",
    "cost = tf.reduce_mean(tf.square(H - Y))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "for step in range(2001):\n",
    "    x_batch, y_batch = sess.run([train_x_batch, train_y_batch])\n",
    "    cost_val, h_val, _ = sess.run([cost, H, train], \n",
    "                                 feed_dict={X: x_batch, Y: y_batch})\n",
    "    if step % 100 == 0:\n",
    "        print(step, \"Cost: \", cost_val, \"\\nPrediction\\n\", h_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
