{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial on Deep Network with ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.695804 [[ 0.613177    0.65752822  0.00304974 -0.06903291 -0.26136369]\n",
      " [ 0.26692089 -0.6627562  -0.60580134 -0.44525242  0.96988976]] [[ 0.18295483 -0.91926956  0.50553226  0.84672451]\n",
      " [-0.15527514  0.46524906  0.79821903  0.87687379]\n",
      " [-0.90465909  0.28480244 -0.726982    0.02441068]\n",
      " [-0.93384647 -0.34278798  0.76770687 -0.28675365]\n",
      " [-0.76883435 -0.59093428  0.11928904  0.98940015]]\n",
      "10000 0.00106637 [[ 1.42713511  0.32605517  0.00153462 -0.06903291  0.26808572]\n",
      " [ 0.41727743 -0.6627562  -0.60580134 -0.44525242  1.24461389]] [[ 0.27995691 -0.91926956  0.89335179  1.36300063]\n",
      " [-0.06912535  0.46524906  0.71530104  0.77405179]\n",
      " [-0.90465498  0.28480244 -0.7269842   0.02440594]\n",
      " [-0.93384647 -0.34278798  0.76770687 -0.28675365]\n",
      " [-0.85427004 -0.59093428  0.10971641  1.21243703]]\n",
      "20000 0.000520183 [[ 1.43602502  0.32605517  0.00153462 -0.06903291  0.27491823]\n",
      " [ 0.42041475 -0.6627562  -0.60580134 -0.44525242  1.25192893]] [[ 0.27698845 -0.91926956  0.8987568   1.37103605]\n",
      " [-0.06912535  0.46524906  0.71530104  0.77405179]\n",
      " [-0.90465498  0.28480244 -0.7269842   0.02440594]\n",
      " [-0.93384647 -0.34278798  0.76770687 -0.28675365]\n",
      " [-0.86019015 -0.59093428  0.11258949  1.21705174]]\n",
      "30000 0.000343767 [[ 1.4412576   0.32605517  0.00153462 -0.06903291  0.27883044]\n",
      " [ 0.42217061 -0.6627562  -0.60580134 -0.44525242  1.25591993]] [[ 0.27540839 -0.91926956  0.90196115  1.37564576]\n",
      " [-0.06912535  0.46524906  0.71530104  0.77405179]\n",
      " [-0.90465498  0.28480244 -0.7269842   0.02440594]\n",
      " [-0.93384647 -0.34278798  0.76770687 -0.28675365]\n",
      " [-0.86342007 -0.59093428  0.11421936  1.21965909]]\n",
      "40000 0.000256663 [[ 1.44493115  0.32605517  0.00153462 -0.06903291  0.28159636]\n",
      " [ 0.42333892 -0.6627562  -0.60580134 -0.44525242  1.25872529]] [[ 0.27430198 -0.91926956  0.90414834  1.37892985]\n",
      " [-0.06912535  0.46524906  0.71530104  0.77405179]\n",
      " [-0.90465498  0.28480244 -0.7269842   0.02440594]\n",
      " [-0.93384647 -0.34278798  0.76770687 -0.28675365]\n",
      " [-0.86565959 -0.59093428  0.11521877  1.221524  ]]\n",
      "50000 0.000204724 [[ 1.44764614  0.32605517  0.00153462 -0.06903291  0.28358907]\n",
      " [ 0.42426941 -0.6627562  -0.60580134 -0.44525242  1.26090038]] [[ 0.273514   -0.91926956  0.90577531  1.38134193]\n",
      " [-0.06912535  0.46524906  0.71530104  0.77405179]\n",
      " [-0.90465498  0.28480244 -0.7269842   0.02440594]\n",
      " [-0.93384647 -0.34278798  0.76770687 -0.28675365]\n",
      " [-0.8673752  -0.59093428  0.11606106  1.22304368]]\n",
      "[[  3.35483901e-06]\n",
      " [  9.99593198e-01]\n",
      " [  9.99593198e-01]\n",
      " [  1.77041954e-06]] [[ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]] 1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "x_data = np.array([[0,0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "\n",
    "#\n",
    "with tf.name_scope(\"layer1\") as scope:\n",
    "    W1 = tf.Variable(tf.random_uniform([2, 5], -1.0, 1.0), name=\"Weight1\")\n",
    "    b1 = tf.Variable(tf.zeros([5]), name=\"Bias1\")\n",
    "    L2 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "    \n",
    "    w1_hist = tf.summary.histogram(\"weights1\", W1)\n",
    "    b1_hist = tf.summary.histogram(\"biases1\", b1)\n",
    "    layer1_hist = tf.summary.histogram(\"layer1\", L2)\n",
    "    \n",
    "with tf.name_scope(\"layer2\") as scope:\n",
    "    W2 = tf.Variable(tf.random_uniform([5, 4], -1.0, 1.0), name=\"Weight2\")\n",
    "    b2 = tf.Variable(tf.zeros([4]), name=\"Bias2\")\n",
    "    L3 = tf.nn.relu(tf.matmul(L2, W2) + b2)\n",
    "    \n",
    "    w2_hist = tf.summary.histogram(\"weights1\", W2)\n",
    "    b2_hist = tf.summary.histogram(\"biases1\", b2)\n",
    "    layer2_hist = tf.summary.histogram(\"layer1\", L3)\n",
    "    \n",
    "with tf.name_scope(\"layer3\") as scope:\n",
    "    W3 = tf.Variable(tf.random_uniform([4, 4], -1.0, 1.0), name=\"Weight3\")\n",
    "    b3 = tf.Variable(tf.zeros([4]), name=\"Bias3\")\n",
    "    L4 = tf.nn.relu(tf.matmul(L3, W3) + b3)\n",
    "    \n",
    "    w3_hist = tf.summary.histogram(\"weights1\", W3)\n",
    "    b3_hist = tf.summary.histogram(\"biases1\", b3)\n",
    "    layer3_hist = tf.summary.histogram(\"layer1\", L4)\n",
    "    \n",
    "with tf.name_scope(\"layer4\") as scope:\n",
    "    W4 = tf.Variable(tf.random_uniform([4, 4], -1.0, 1.0), name=\"Weight3\")\n",
    "    b4 = tf.Variable(tf.zeros([4]), name=\"Bias3\")\n",
    "    L5 = tf.nn.relu(tf.matmul(L4, W4) + b4)\n",
    "        \n",
    "with tf.name_scope(\"layer5\") as scope:\n",
    "    W5 = tf.Variable(tf.random_uniform([4, 4], -1.0, 1.0), name=\"Weight3\")\n",
    "    b5 = tf.Variable(tf.zeros([4]), name=\"Bias3\")\n",
    "    L6 = tf.nn.relu(tf.matmul(L5, W5) + b5)\n",
    "        \n",
    "with tf.name_scope(\"layer6\") as scope:\n",
    "    W6 = tf.Variable(tf.random_uniform([4, 4], -1.0, 1.0), name=\"Weight3\")\n",
    "    b6 = tf.Variable(tf.zeros([4]), name=\"Bias3\")\n",
    "    L7 = tf.nn.relu(tf.matmul(L6, W6) + b6)\n",
    "    \n",
    "with tf.name_scope(\"last\") as scope:\n",
    "    W7 = tf.Variable(tf.random_uniform([4, 1], -1.0, 1.0), name=\"Weight4\")\n",
    "    b7 = tf.Variable(tf.zeros([1]), name=\"Bias4\")\n",
    "    H  = tf.sigmoid(tf.matmul(L7, W7) + b7)\n",
    "    \n",
    "with tf.name_scope(\"cost\") as scope:    \n",
    "    cost = -tf.reduce_mean(Y * tf.log(H) + (1-Y)*tf.log(1-H))\n",
    "    cost_summ = tf.summary.scalar(\"cost\", cost)\n",
    "\n",
    "\n",
    "with tf.name_scope(\"train\") as scope:\n",
    "        train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "predicted = tf.cast(H > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "with tf.Session() as sess:    \n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(\"./logs/xor_logs_r0_01\")\n",
    "    writer.add_graph(sess.graph)  # Show the graph\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    \n",
    "    for step in range(50001):\n",
    "        cost_val, W1_val, W2_val, _ = sess.run([cost, W1, W2, train], feed_dict = {X: x_data, Y: y_data})\n",
    "        if step % 10000 == 0:\n",
    "            print(step, cost_val, W1_val, W2_val)\n",
    "            \n",
    "    h, c, a = sess.run([H, predicted, accuracy], feed_dict = {X: x_data, Y: y_data})\n",
    "    print(h, c, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
